"""
@author: heet
"""
import pandas as pd
import numpy as np
import copy

def load_metrics(ksdir):
    """load metrics and driftQC csv files. ksdir must have a metrics.csv which
    is generated by the pre-processing pipeline and a driftQC.csv containing the 
    drift metric. driftQC.csv is a single column file with no headers.
    
    Returns a dataframe containing all metrics and additional meta
    
    NOTE: If metrics.csv already has a column named 'drift' then driftQC.csv will
    not be used. 
    """
    metrics = pd.read_csv(ksdir + '/metrics.csv', sep=',')
    if not 'drift' in metrics: #load drift from drfitQC.csv if not already present in metrics.csv
        metrics['drift'] = pd.read_csv(ksdir + '/driftQC.csv', header=None, 
                                       names=['drift'])
    metrics['kslabel'] = pd.read_csv(ksdir + '/cluster_KSLabel.tsv', sep='\t',
                                     usecols=['KSLabel'])
    labels = pd.read_csv(ksdir + '/cluster_group.tsv', sep='\t')
    labels = labels[labels['group'].isin(['mua', 'good'])]
    result = metrics[['cluster_id', 'firing_rate', 'presence_ratio', 'isi_viol',
            'amplitude_cutoff', 'isolation_distance', 'l_ratio',
            'd_prime', 'nn_hit_rate', 'nn_miss_rate', 'silhouette_score',
            'max_drift', 'cumulative_drift', 'snr', 'amplitude', 'drift', 
            'kslabel']].copy()
    result['manual_labels'] = -1
    good_clusters = labels['cluster_id'].to_numpy()
    vals = labels['group'].to_numpy()
    for cid in result['cluster_id']:
        if cid in good_clusters:
            ix = np.where(good_clusters == cid)[0]
            val = vals[ix][0]
            ix = result[result['cluster_id']==cid].index[0]
            result.loc[ix, 'manual_labels'] = 1 if val == 'good' else 0
    result['energy'] = get_energy(metrics['cluster_id'], ksdir)
    return result

def get_energy(cluster_id, ksdir):
    """Compute waveform energy for given cluster IDs
    
    Args:
        cluster_id (list-like):  list of cluster IDs to compute for
        ksdir (str):             Kilosort output directory
        
    Returns an array of energies
    """
    mean_wfs = np.load(ksdir + '\mean_waveforms.npy')
    mean_wfs_squared = mean_wfs**2
    
    energy = []
    for wf in mean_wfs_squared:
        s_time = wf.sum(axis=1) #sum across time
        s_chs = s_time.sum() #sum across channels
        energy.append(s_chs)
    energy = np.array(energy)
    
    return energy[cluster_id]

def get_metrics(ksdirs_csv, dropna=True):
    """Get metrics from multiple sessions.
    
    Args:
        ksdirs_csv (str):       Path to CSV containing a list of kilosort output
                                folders. See sample folder for an example
        dropna (boolean):       Falg to drop samples with missing metrics
    
    Returns a contcatenated dataframe with metrics from all sessions and a list
    of dir paths for those sessions
    """
    ksdirs = pd.read_excel(ksdirs_csv)
    metrics = []
    for ix, ksdir in enumerate(ksdirs.Path):
        metric_df = load_metrics(ksdir)
        metric_df['animal_id'] = ksdirs.Animal_ID[ix]
        metric_df['session'] = ksdirs.Session[ix]
        metric_df['probe_type'] = ksdirs.NP[ix]
        metric_df['date'] = ksdirs.Date[ix]
        metric_df['imec'] = ksdirs.Imec[ix]
        metric_df['uid'] = ksdirs.UID[ix]
        metrics.append(metric_df)
    res = pd.concat(metrics).dropna() if dropna else pd.concat(metrics)
    return res, ksdirs.Path.to_list()   
    
def trim_NPYs(cluster_id, ksdir, truncation_meta_path, save_dir):
    """Trim Kilosort NPY files based on cluster IDs
    
    Args:
        cluster_id (list-like):         cluster IDs to keep
        ksdir (str):                    Kilosort output directory
        truncation_meta_path (str):     Path to meta info about truncation
        save_dir (str):                 save directory for trimmed files.
    
    Returns a list of tuple pairs containing (old cluster id, new cluster id)
    
    NOTE: 1. Cluster IDs are re-arranged after trimming. New IDs start from 0.
          2. The mapping of old IDs to new ones is returned by this function
    """
    if save_dir != ksdir:
        spike_clusters = np.load(ksdir + '\spike_clusters.npy')
        spike_ix = [np.where(spike_clusters == c)[0] for c in cluster_id]
        spike_ix = np.sort(np.concatenate(spike_ix))
        
        #rename clusters starting from 0
        spike_clusters = spike_clusters[spike_ix]
        unique_clus = np.sort(np.unique(spike_clusters)) #TODO: should be same as cluster_id
        tokens = [(c, ix) for ix, c in enumerate(unique_clus)]
        new_spike_clusters = replace(spike_clusters, tokens)
        np.save(save_dir + '/spike_clusters.npy' , new_spike_clusters)
        
        truncate_meta = pd.read_excel(truncation_meta_path, 
                                      converters={'axis':str, 're_order':str,
                                                  're_order_col':str})
        
        for index, row in truncate_meta.iterrows():
            fname = row['fname']
            index_type = row['index_type']
            axis = row['axis']
            re_order = row['re_order']
            trim = row['trim']
            print("File: {}" .format(fname))
            x = np.load(ksdir + '/' + fname)
            if trim:
                ix = spike_ix if index_type == 'spikes' else cluster_id
                for a in axis.split(','):
                    x = np.take(x, ix, axis=int(a))
                if re_order == 'True':
                    for col in row['re_order_col'].split(','):
                        col = int(col)
                        x[:, col] = replace(x[:, col], tokens)
                
                if 'template_feature_ind' in fname:
                    for i, col in enumerate(x.T):
                        x[:, i] = replace(col, tokens)
                
            np.save(save_dir + '/' + fname, x)
        return tokens
    else:
        print("Warning: kilosort directory is same as save directory!")

def replace(arr, tokens):
    """Replace element(s) in 1D array with new value(s) 
    
    Args:
        arr (1d array):     1-d numpy array
        tokens (list):      list of tuples containing old and new value
    NOTE: Makes a deep copy of arr. Does NOT replace in place.
    """
    res = copy.deepcopy(arr)
    for a, b in tokens:
        res[np.where(arr == a)[0]] = b
    return res

def extract(df, **filters):
    """Extract rows from DataFrame using filters
    
    Args:
        df (DataFrame):     Pandas dataframe
        filters (dict):     column headers and filter values
    
    Returns a filtered copy of the original dataframe
    """
    filtered_df = df.copy()
    for key, val in filters.items():
        filtered_df = filtered_df[filtered_df[key].isin(val)]
    return filtered_df

def extract_by_thresholds(df, greater_equal={}, less_equal={}):
    """Extract rows from DataFrame using CONDITIONAL filters
    
    Args:
        df (DataFrame):         Pandas dataframe
        greater_equal (dict):   column headers and filter values for '>='
        less_equal (dict):      column headers and filter values for '<=''
    Returns a filtered copy of the original dataframe
    """
    filtered_df = df.copy()
    for key, val in greater_equal.items():
        filtered_df = filtered_df[filtered_df[key] >= val]
    for key, val in less_equal.items():
        filtered_df = filtered_df[filtered_df[key] <= val]
    return filtered_df